# ğŸµ SHL Hiring Assessment (Audio Feature Extraction and Grammar Score Prediction)

# ğŸµ Audio Grammar Scoring

This repository contains code, analysis, and models for predicting grammar quality scores from raw `.wav` audio files using audio feature extraction techniques and machine learning.

ğŸ“„ **Note:** The detailed report explaining the approach, preprocessing steps, pipeline architecture, evaluation results, and interpretation of visualizations is available in the `Report/` folder.



---


## ğŸ” Approach Overview

### ğŸ”¸ Input Audio
- Raw `.wav` audio files are used as input.

### ğŸ”¸ Feature Extraction using Librosa
- MFCCs (13 coefficients)
- Chroma features
- Spectral contrast
- Zero Crossing Rate (ZCR)
- RMSE energy
- Spectral Rolloff
- Tempo

### ğŸ”¸ Feature Matrix
- All extracted features are combined into a matrix.

### ğŸ”¸ Preprocessing
- Standard scaling (mean = 0, std = 1)

### ğŸ”¸ Train/Test Split
- 80% training / 20% testing

### ğŸ”¸ Models Used
- Random Forest (balanced and unbalanced)
- XGBoost
- Gradient Boosting

### ğŸ”¸ Evaluation Metrics
- Pearson Correlation (r = 0.648)
- RÂ² Score (0.395)
- RMSE (0.908)

---

## ğŸ”¢ Exploratory Data Analysis (EDA)

### 1. Grammar Score Distribution
- **Most Common:** 5.0  
- **Rare Scores:** 1.0, 1.5, 2.0  
- **Skew:** Right-skewed  
> ğŸ“Œ Imbalance suggests use of class weighting or data augmentation.

### 2. Audio Duration
- **Majority:** < 20 seconds  
- **Outliers:** > 60 seconds  
> ğŸ“Œ Outliers might affect model learning.

### 3. Sample Rate
- **Uniform:** 16 kHz  
> ğŸ“Œ Ideal for speech processing.

---

## ğŸ¼ Feature Extraction (Librosa)

| Feature           | Description                                                         |
|-------------------|---------------------------------------------------------------------|
| **MFCCs**         | Capture timbral & phonetic patterns                                 |
| **Chroma**        | Highlight harmonic content                                          |
| **Spectral Contrast** | Distinguish voiced/unvoiced transitions, articulation quality |
| **ZCR**           | Measure noisiness/disfluencies                                      |
| **RMSE**          | Quantify energy; reflects emphasis or emotion                       |
| **Spectral Rolloff** | Detects sharp consonants; spectral energy threshold             |
| **Tempo**         | Measures rhythm/fluency of speech                                   |

---

## ğŸ“Š Feature-Label Insights

### âœ© ZCR (Zero Crossing Rate)
- **Low scores** â†’ High ZCR â†’ Noisy/disfluent speech  
- **High scores** â†’ Low ZCR â†’ Clear articulation

### âœ© RMSE (Energy)
- **Low scores** â†’ Variable RMSE â†’ Unstable/emotional tone  
- **High scores** â†’ Stable RMSE â†’ Controlled delivery

### âœ© Spectral Rolloff
- **Low scores** â†’ High rolloff â†’ Harsh articulation  
- **High scores** â†’ Low rolloff â†’ Balanced speech spectrum

---

## âš™ï¸ Modeling Pipeline

### ğŸ§ª Preprocessing
- Feature scaling using `StandardScaler`
- Dataset split: 80% training / 20% testing

### ğŸ§  Model Training & Evaluation

| Model                    | Pearson (r) | RÂ² Score | RMSE  | MAE    | MAPE   |
|--------------------------|-------------|----------|--------|--------|--------|
| Random Forest (unbalanced) | 0.634       | 0.383    | 0.917 | ~0.75 | ~25%  |
| Random Forest (balanced)   | **0.648**   | **0.395**| 0.908 | ~0.75 | ~25%  |
| XGBoost                    | Lower performance across all metrics |

> âœ… **Best Model:** Balanced Random Forest with `GridSearchCV`

---

## ğŸ§² Evaluation Metric

### ğŸ“Œ Pearson Correlation Coefficient (r)
- Measures linear correlation between predicted and actual grammar scores  
- **Primary evaluation metric used**

## ğŸ“ Requirements

- **Python**: 3.x

### ğŸ“¦ Libraries

```txt
numpy  
pandas  
matplotlib  
scikit-learn  
xgboost  
librosa

---

## ğŸ¦ Dataset Structure

```bash
dataset/
â”œâ”€â”€ audio_files/
â”‚   â”œâ”€â”€ audio_1.wav
â”‚   â”œâ”€â”€ audio_2.wav
â”‚   â””â”€â”€ ...
â”œâ”€â”€ train.csv                 # Training data with filenames and grammar labels
â”œâ”€â”€ test.csv                  # Test data with filenames and dummy labels
â””â”€â”€ sample_submission.csv     # Submission format

